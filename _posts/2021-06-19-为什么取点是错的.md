---
layout:		post
title:		为什么取点是错的
date:		2021-06-19
author:		wyj
catalog:	true
tags:
    - 数学
    - 文化课
---

高考都过去这么久了，我才忽然想起来这个事（~~前几天颓得太厉害了~~），但觉得不特别说明一下还是不太好，所以特地写了这一篇短文。

> 已知$f'(a)>0$，求证$\exists x>a,f(x)>f(a)$

这是个显然的事，由极限的保号性立即可得。然而在高中数学中一般来讲需要你取一个$b>a$，使得$f'(b)<0$，然后再说明存在一个$f'(x_0)=0$，取最小的一个$x_0$，所以函数在$[a,x_0]$内单调增，然后得证。

在之前的[文章](/2021/04/30/%E6%95%B4%E6%95%B0%E7%BA%BF%E6%80%A7%E9%80%92%E6%8E%A8%E6%9E%81%E9%99%90/)里面我提到过，我很讨厌取点。然而没办法，因为在高中数学的框架之中真的就是没办法说明这个事的。这不只是很麻烦而且没必要的问题，**实际上这个论证思路完全就是错的**。错误的本质在于，你的手段是通过找到一个“单调”的区间来说明问题，而这不一定可行。错误的直接原因是，的确存在$f'(x_0)=0$，但是“最小的”并不一定存在。实数又不是自然数。

如果大家还记得“导数一定连续吗”的那个反例，我们可以很轻松地构造出一个反例来驳倒上面的歪理：$f(x)=x^2\sin\dfrac{1}{x}+\dfrac{x}{2}$，定义$f(0)=0$去掉可去的间断点，取$a=0$，根据定义显然$f'(0)=\dfrac12>0$，然而显然存在任意小的$\epsilon>0$使得$f'(\epsilon)=0$，取不出最小的，所以推理是错误的。对于这个函数来讲，你想要找的那个“单调区间”，压根就不存在。

无论你取不取点，只要你的思路最后回到“找单调区间”上，就是错的。所以其它的一些“说理”，无论是否取了点，都是错的。

当然，欢迎任何人找到用高中知识正确说明这个问题的方法来打我的脸。毕竟我没办法证明“满足条件的说理不存在”。
